<h1 align="center">ğŸ¤– Reconhecimento de PadrÃµes</h1>

<p align="center">
RepositÃ³rio da disciplina/projeto de <strong>Reconhecimento de PadrÃµes</strong> com foco em 
prÃ©â€‘processamento de dados, classificaÃ§Ã£o, agrupamento e avaliaÃ§Ã£o de modelos.  
Organizado para estudos, experimentos reprodutÃ­veis e comparaÃ§Ã£o de algoritmos.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Ã¡rea-Reconhecimento%20de%20PadrÃµes-purple" alt="Ãrea">
  <img src="https://img.shields.io/badge/finalidade-acadÃªmica-blue" alt="AcadÃªmico">
</p>

> **Disciplina:** Reconhecimento de PadrÃµes  
> **Objetivo:** Estudo e compreensÃ£o dos principais mÃ©todos de classificaÃ§Ã£o e agrupamento  
> **Uso:** PortfÃ³lio acadÃªmico e material de apoio

---

## ğŸ“˜ DescriÃ§Ã£o

Este repositÃ³rio reÃºne conteÃºdos e experimentos desenvolvidos ao longo da disciplina de
**Reconhecimento de PadrÃµes**, com o objetivo de compreender como diferentes algoritmos
podem identificar padrÃµes, classificar dados e agrupar amostras semelhantes.

O foco Ã© **conceitual**, servindo como material de apoio e portfÃ³lio acadÃªmico.

---

## ğŸ“Š Estimadores de Densidade ProbabilÃ­stica

Estimadores de densidade sÃ£o utilizados para modelar a distribuiÃ§Ã£o dos atributos em cada classe,
permitindo o cÃ¡lculo da probabilidade de ocorrÃªncia de um determinado valor.

---

### ğŸ”¹ Estimador Gaussiano

O **Estimador Gaussiano** assume que os dados seguem uma **distribuiÃ§Ã£o normal**.
Para cada atributo de cada classe, sÃ£o calculados a **mÃ©dia** e o **desvio padrÃ£o**,
utilizados na funÃ§Ã£o densidade da normal:

\[
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \cdot e^{-\frac{(x - \mu)^2}{2\sigma^2}}
\]

Onde:

- `x` = valor da amostra
- `Î¼` = mÃ©dia
- `Ïƒ` = desvio padrÃ£o

âœ”ï¸ Simples e eficiente  
âŒ Dependente da suposiÃ§Ã£o de normalidade

---

### ğŸ”¹ Kernel Density Estimation (KDE)

O **KDE** Ã© um estimador **nÃ£o paramÃ©trico**, que nÃ£o assume uma forma fixa para a distribuiÃ§Ã£o dos dados.
A densidade Ã© construÃ­da a partir da soma de **kernels** centrados em cada amostra:

\[
\hat{f}(x) = \frac{1}{n h} \sum\_{i=1}^{n} K\left(\frac{x - x_i}{h}\right)
\]

Para o **kernel Gaussiano**:

\[
K(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}}
\]

Resultando em:

\[
\hat{f}(x) = \frac{1}{n h \sqrt{2\pi}} \sum\_{i=1}^{n} e^{-\frac{1}{2}\left(\frac{x - x_i}{h}\right)^2}
\]

Onde:

- `n` = nÃºmero de amostras
- `h` = largura de banda (_bandwidth_)
- `xáµ¢` = amostras do conjunto de treino

âœ”ï¸ Modela distribuiÃ§Ãµes complexas  
âŒ Custo computacional maior e dependente de `h`

---

## ğŸ“Œ Classificador Bayesiano

O **Classificador Bayesiano** utiliza o **Teorema de Bayes** para estimar a probabilidade de uma amostra
pertencer a uma determinada classe, a partir de seus atributos.

---

### ğŸ“ Teorema de Bayes

\[
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
\]

- `P(C)` â†’ Probabilidade a priori da classe
- `P(X|C)` â†’ Probabilidade da amostra dado a classe
- `P(X)` â†’ Fator de normalizaÃ§Ã£o

---

### âš™ï¸ Naive Bayes

O **Naive Bayes** assume independÃªncia entre os atributos:

\[
P(X|C) = \prod\_{i=1}^{n} P(X_i | C)
\]

Assim:

\[
P(C|X) \propto P(C) \cdot \prod\_{i=1}^{n} P(X_i | C)
\]

ğŸ“Œ A classe atribuÃ­da Ã© aquela que maximiza `P(C|X)`.

âœ”ï¸ Simples e eficiente  
âŒ SensÃ­vel Ã  correlaÃ§Ã£o entre atributos

---

## ğŸ“Š Base de Dados Utilizada

A base **transfusion.data** contÃ©m informaÃ§Ãµes sobre doadores de sangue
e Ã© utilizada para prever se um doador realizarÃ¡ uma nova doaÃ§Ã£o em determinado perÃ­odo.

- **Total de registros:** 748
- **NÃºmero de atributos:** 5

| Atributo      | DescriÃ§Ã£o                               |
| ------------- | --------------------------------------- |
| Recency (R)   | Meses desde a Ãºltima doaÃ§Ã£o             |
| Frequency (F) | Total de doaÃ§Ãµes realizadas             |
| Monetary (M)  | Volume total doado (c.c.)               |
| Time (T)      | Meses desde a primeira doaÃ§Ã£o           |
| Class         | DoaÃ§Ã£o em marÃ§o/2007 (1 = sim, 0 = nÃ£o) |

âš ï¸ As classes sÃ£o **desbalanceadas**, com maioria de exemplos negativos.

---

## ğŸ§  Exemplos de ImplementaÃ§Ã£o

- **Estimador Gaussiano** â€” abordagem paramÃ©trica
- **KDE** â€” abordagem nÃ£o paramÃ©trica
- **Naive Bayes** â€” combinaÃ§Ã£o probabilÃ­stica dos estimadores

---

## ğŸ’» Requisitos

- **Java JDK 17+**
- **Swing** (seleÃ§Ã£o de arquivos)
- Arquivos `.data` de treino e teste

---

## â–¶ï¸ Uso

1. Compile o projeto:

```bash
javac *.java
```
